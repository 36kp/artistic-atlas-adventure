{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subfolders in root folder \n",
    "root_folder = r\"Resources\"\n",
    "sub_folder_names = os.listdir(root_folder)\n",
    "sub_folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list for dataframes\n",
    "df_list = []\n",
    "\n",
    "# make dataframes for each subfolder and label each image\n",
    "for sub_folder_name in sub_folder_names:\n",
    "    subfolder_path = os.path.join(root_folder, sub_folder_name)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue  # Skip if it's not a directory // such as .DS_Store\n",
    "    else:\n",
    "        file_names = os.listdir(subfolder_path)\n",
    "        df = pd.DataFrame(file_names, columns=['image_id'])\n",
    "        df['label'] = sub_folder_name\n",
    "        df_list.append(df)\n",
    "\n",
    "# concat list of dataframes into one dataframe\n",
    "concat_df = pd.concat(df_list)\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are null values\n",
    "concat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all image ids are unique\n",
    "len(concat_df['image_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts for labels\n",
    "concat_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph value counts\n",
    "concat_df['label'].value_counts().sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to store images\n",
    "imgs = []\n",
    "# create empty list to image formats\n",
    "imgs_format = []\n",
    "\n",
    "# iterate through each row and get file path for each img\n",
    "# open image and append it to empty list\n",
    "for row in concat_df.itertuples():\n",
    "    file_path = os.path.join(root_folder, row.label, row.image_id)\n",
    "    with Image.open(file_path) as img:\n",
    "        imgs_format.append(img.format)\n",
    "        imgs.append(img.copy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all images in same file format\n",
    "set(imgs_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique image sizes\n",
    "sizes = set([img.size for img in imgs])\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary to store img size counts\n",
    "img_size_count = {}\n",
    "\n",
    "# get counts for each image size\n",
    "for img in imgs:\n",
    "    if str(img.size) in img_size_count:\n",
    "        img_size_count[str(img.size)] += 1\n",
    "    else:\n",
    "        img_size_count[str(img.size)] = 1\n",
    "\n",
    "# print to determine which image size that is most abundant in dataset\n",
    "img_size_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use target size that is most abundant in dataset \n",
    "target_size = (150, 150)\n",
    "\n",
    "# resize images\n",
    "resized_imgs = [img.resize(target_size, resample = Image.LANCZOS) for img in imgs]\n",
    "resized_imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if resizing images was succesful by checking unique values again\n",
    "sizes = set([img.size for img in resized_imgs])\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all images are in RGB format\n",
    "set([img.mode for img in resized_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all images to floating point numpy arrays\n",
    "float_images = [np.array(img).astype(np.float32) for img in resized_imgs]\n",
    "\n",
    "# Display the pixel values of the first image\n",
    "print(\"Pixel Values:\")\n",
    "print(float_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to a range between 0 and 1,\n",
    "# divide all pixel values by the max of 255\n",
    "normalized_images = [img/255 for img in float_images]\n",
    "\n",
    "# Display the pixel values of the first image\n",
    "print(\"Pixel Values:\")\n",
    "print(normalized_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(normalized_images)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = concat_df['label']\n",
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the y data\n",
    "y_encoder = LabelEncoder().fit(y)\n",
    "y = y_encoder.transform(y)\n",
    "set(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, frequency = np.unique(y_train_aug, \n",
    "#                               return_counts = True)\n",
    "\n",
    "# print(unique)\n",
    "# print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, frequency = np.unique(y_test, \n",
    "#                               return_counts = True)\n",
    "\n",
    "# print(unique)\n",
    "# print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the augmentation pipeline\n",
    "# data_augmentation = tf.keras.Sequential([\n",
    "#     tf.keras.layers.RandomRotation(0.05),        # Random rotation (20 degrees)\n",
    "#     tf.keras.layers.RandomZoom(0.2),             # Random zoom\n",
    "#     tf.keras.layers.RandomFlip('horizontal')     # Random horizontal flip\n",
    "\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list for both X and y augmentations\n",
    "# X_train_aug = []\n",
    "# y_train_aug = []\n",
    "\n",
    "# # Loop through each image in the training data\n",
    "# for i in range(len(X_train)):\n",
    "#     # Select the image and its y label\n",
    "#     img = X_train[i]\n",
    "#     label = y_train[i]\n",
    "\n",
    "#     # Add the batch dimension\n",
    "#     img = np.expand_dims(img, axis=0)\n",
    "\n",
    "#     # Use a loop to create 2 new images\n",
    "#     # Append each to X_train_aug\n",
    "#     # For each image, append the correct label to y_train_aug\n",
    "#     for j in range(1):\n",
    "#         X_train_aug.append(data_augmentation(img, training=True)[0].numpy())\n",
    "#         y_train_aug.append(label)\n",
    "\n",
    "# # Print the lengths of both augmented sets to ensure they are the same length\n",
    "# print(len(X_train_aug))\n",
    "# print(len(y_train_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_test.pkl', 'wb') as file:\n",
    "#     pickle.dump(X_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('y_test.pkl', 'wb') as file:\n",
    "#     pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_aug.pkl', 'rb') as file:\n",
    "    X_train_aug = pickle.load(file)\n",
    "\n",
    "with open('y_train_aug.pkl', 'rb') as file:\n",
    "    y_train_aug = pickle.load(file)\n",
    "\n",
    "\n",
    "with open('y_test.pkl', 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "\n",
    "with open('X_test.pkl', 'rb') as file:\n",
    "    X_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the original and augmented images\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for i in range(3):\n",
    "#     plt.subplot(1, 3, i + 1)\n",
    "#     if i == 0:\n",
    "#         plt.imshow((reshaped_image_array[0, :, :, 0]*255).astype('uint8'), cmap='gray')  # Original image\n",
    "#     else:\n",
    "#         plt.imshow((augmented_images[i - 1][:, :, 0]*255).astype('uint8'), cmap='gray')\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to numpy arrays\n",
    "X_train_aug_np = np.array(X_train_aug)\n",
    "X_test_np = np.array(X_test)\n",
    "y_train_aug_np = np.array(y_train_aug)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    # layers.MaxPooling2D((2, 2)),\n",
    "    # layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')  #6 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# batch_size = 32\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size= 32\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
